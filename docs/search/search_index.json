{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LetsCheck Project layout Deployment overview LetsCheck website Twatch Twatch application Tweets crawler Quin Quin-COVID Quin-general API request handling","title":"LetsCheck"},{"location":"#letscheck","text":"","title":"LetsCheck"},{"location":"#project-layout","text":"Deployment overview LetsCheck website Twatch Twatch application Tweets crawler Quin Quin-COVID Quin-general API request handling","title":"Project layout"},{"location":"api-request-handling/","text":"API request handling The NUS VM web server hosting the LetsCheck HTML pages lacks the resources needed to host the dataset and run models directly. Therefore, the backend applications are deployed on machines located in the IDS office. However, these IDS machines cannot be accessed directly from the NUS VM. To work around this, both Twatch and Quin have their own API servers with exposed endpoints to facilitate communication with the backend applications. The diagram below explains how this setup functions. The request handling process is implemented in Twatch/backend-api and Quin/frontend. The difference in naming exists due to legacy reasons.","title":"API request handling"},{"location":"api-request-handling/#api-request-handling","text":"The NUS VM web server hosting the LetsCheck HTML pages lacks the resources needed to host the dataset and run models directly. Therefore, the backend applications are deployed on machines located in the IDS office. However, these IDS machines cannot be accessed directly from the NUS VM. To work around this, both Twatch and Quin have their own API servers with exposed endpoints to facilitate communication with the backend applications. The diagram below explains how this setup functions. The request handling process is implemented in Twatch/backend-api and Quin/frontend. The difference in naming exists due to legacy reasons.","title":"API request handling"},{"location":"deployment-overview/","text":"LetsCheck deployment overview LetsCheck Website The public-facing LetsCheck website is a static HTML site hosted on an NUS VM using an Apache web server. The site features demos for two projects: LetsCheck COVID and LetsCheck General Search. LetsCheck COVID is the original fact-checking project, divided into three sections that handle different information sources: Twitter, news, and scientific articles. The first section, Twatch, processes and analyzes COVID-related Twitter data. The other two sections use Quin, a model designed to fact-check statements against datasets, in this case, COVID-related news and scientific articles. LetsCheck General Search is based on a modified version of the Quin-general model. It first conducts a general web search for the query, then uses the Quin model to fact-check the results from the search. Twatch Twatch offers a searchable archive of COVID-related claims circulating on X (formerly Twitter) from 2020 to 2023, along with visualizations showing their impact. Twatch consists of the following components: Frontend : A web app built using the React library. Backend-API : For more details, see this page . Backend (Data) : A Python script that retrieves data from the tweet database. Crawler : Downloads tweet data from the Panacea Lab dataset . This component is currently inactive as the dataset is no longer being updated. Quin Quin is a user-friendly framework designed for large-scale fact-checking and question answering. You can explore it on the Github repository . LetsCheck offers demos for two Quin models: Quin-COVID and Quin-web. Quin-COVID : Two separate Quin-COVID instances are deployed on a DGX A100 workstation. One instance is based on a dataset of news articles, while the other uses scientific articles from the COVID-19 Open Research Dataset . Quin-general : An upgraded version of Quin that supports general queries and conducts fact-checking using web search results. Quin-frontend : For more details, refer to this page .","title":"Overview"},{"location":"deployment-overview/#letscheck-deployment-overview","text":"","title":"LetsCheck deployment overview"},{"location":"deployment-overview/#letscheck-website","text":"The public-facing LetsCheck website is a static HTML site hosted on an NUS VM using an Apache web server. The site features demos for two projects: LetsCheck COVID and LetsCheck General Search. LetsCheck COVID is the original fact-checking project, divided into three sections that handle different information sources: Twitter, news, and scientific articles. The first section, Twatch, processes and analyzes COVID-related Twitter data. The other two sections use Quin, a model designed to fact-check statements against datasets, in this case, COVID-related news and scientific articles. LetsCheck General Search is based on a modified version of the Quin-general model. It first conducts a general web search for the query, then uses the Quin model to fact-check the results from the search.","title":"LetsCheck Website"},{"location":"deployment-overview/#twatch","text":"Twatch offers a searchable archive of COVID-related claims circulating on X (formerly Twitter) from 2020 to 2023, along with visualizations showing their impact. Twatch consists of the following components: Frontend : A web app built using the React library. Backend-API : For more details, see this page . Backend (Data) : A Python script that retrieves data from the tweet database. Crawler : Downloads tweet data from the Panacea Lab dataset . This component is currently inactive as the dataset is no longer being updated.","title":"Twatch"},{"location":"deployment-overview/#quin","text":"Quin is a user-friendly framework designed for large-scale fact-checking and question answering. You can explore it on the Github repository . LetsCheck offers demos for two Quin models: Quin-COVID and Quin-web. Quin-COVID : Two separate Quin-COVID instances are deployed on a DGX A100 workstation. One instance is based on a dataset of news articles, while the other uses scientific articles from the COVID-19 Open Research Dataset . Quin-general : An upgraded version of Quin that supports general queries and conducts fact-checking using web search results. Quin-frontend : For more details, refer to this page .","title":"Quin"},{"location":"letscheck-website/","text":"LetsCheck Website The public-facing LetsCheck website is a static HTML site, hosted on NUS VM with Apache web server. The details of the NUS VM are: Server name: lxwebprod05012.res.nus.edu.sg IP: 137.132.174.114 The SSL certificates are managed through nCertRequest. For any issues regarding NUS VM or certificate, please contact NUS IT UNIX System Administrators NUSITUNIXSystemAdministrators@nus.edu.sg with the information above. Notes Ensure there are no inline scripts in html files; it is forbidden by the NUS VM content security policy . Put JS in the corresponding .js files instead.","title":"LetsCheck website"},{"location":"letscheck-website/#letscheck-website","text":"The public-facing LetsCheck website is a static HTML site, hosted on NUS VM with Apache web server. The details of the NUS VM are: Server name: lxwebprod05012.res.nus.edu.sg IP: 137.132.174.114 The SSL certificates are managed through nCertRequest. For any issues regarding NUS VM or certificate, please contact NUS IT UNIX System Administrators NUSITUNIXSystemAdministrators@nus.edu.sg with the information above.","title":"LetsCheck Website"},{"location":"letscheck-website/#notes","text":"Ensure there are no inline scripts in html files; it is forbidden by the NUS VM content security policy . Put JS in the corresponding .js files instead.","title":"Notes"},{"location":"quin-covid/","text":"Quin-COVID The Quin-COVID backend model currently runs on the DGX A100 workstation. It requires at least 8GB RAM and one GPU with 4GB+ VRAM. Quick start The Quin-COVID process is currently managed as a system service, with the script located at /etc/init.d/quin-covid . To check if Quin-COVID is running, run the following command: /etc/init.d/quin-covid status If it is not running, you can start the service by running: /etc/init.d/quin-covid start To run the script directly for testing, change the directory to then run quin_covid.py --mode serve Set up from scratch 1) Download the encoder model from here and put it under models/weights/encoder 2) Download the NLI model from here and put it under models/weights/nli 3) Download the passage ranking model from here and put it under models/weights/passage_ranker 4) Run pip install -r backend/requirements.txt Setting up the GPU In backend/quin_covid.py you can find the following line: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"...\" Depending on which GPUs are available on the machine, you should set this variable to at least one avaiable GPU id. One GPU is sufficient to run the system. For example, if GPU number 3 is available then you can set it as follows: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" A nice way to check the availability of the GPUs is using the gpustat command. You can install this tool using pip: pip install gpustat Dataset preparation There are existing scripts scripts/update-quin-covid.news.sh and scripts/update-quin-covid-research.sh which facilitate dataset updates. The exact steps are explained below: 1) To crawl new articles, run backend/crawler.py to update backend/data/covid-news.jsonl . 2) To create a new dataset of research papers based on the CORD corpus: - Download the latest CORD dataset here . - Place it under backend/data/cord/ . - Run backend/data/cord/build_dataset.py to generate a new backend/data/cord/dataset.jsonl file. 3) Once the data files are in place, run preprocessing/preprocess_encode.py --mode index for scientific articles, or preprocessing/preprocess_encode.py --mode index-news for news, to encode and save the indexed data in the database.","title":"Quin-COVID"},{"location":"quin-covid/#quin-covid","text":"The Quin-COVID backend model currently runs on the DGX A100 workstation. It requires at least 8GB RAM and one GPU with 4GB+ VRAM.","title":"Quin-COVID"},{"location":"quin-covid/#quick-start","text":"The Quin-COVID process is currently managed as a system service, with the script located at /etc/init.d/quin-covid . To check if Quin-COVID is running, run the following command: /etc/init.d/quin-covid status If it is not running, you can start the service by running: /etc/init.d/quin-covid start To run the script directly for testing, change the directory to then run quin_covid.py --mode serve","title":"Quick start"},{"location":"quin-covid/#set-up-from-scratch","text":"1) Download the encoder model from here and put it under models/weights/encoder 2) Download the NLI model from here and put it under models/weights/nli 3) Download the passage ranking model from here and put it under models/weights/passage_ranker 4) Run pip install -r backend/requirements.txt","title":"Set up from scratch"},{"location":"quin-covid/#setting-up-the-gpu","text":"In backend/quin_covid.py you can find the following line: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"...\" Depending on which GPUs are available on the machine, you should set this variable to at least one avaiable GPU id. One GPU is sufficient to run the system. For example, if GPU number 3 is available then you can set it as follows: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" A nice way to check the availability of the GPUs is using the gpustat command. You can install this tool using pip: pip install gpustat","title":"Setting up the GPU"},{"location":"quin-covid/#dataset-preparation","text":"There are existing scripts scripts/update-quin-covid.news.sh and scripts/update-quin-covid-research.sh which facilitate dataset updates. The exact steps are explained below: 1) To crawl new articles, run backend/crawler.py to update backend/data/covid-news.jsonl . 2) To create a new dataset of research papers based on the CORD corpus: - Download the latest CORD dataset here . - Place it under backend/data/cord/ . - Run backend/data/cord/build_dataset.py to generate a new backend/data/cord/dataset.jsonl file. 3) Once the data files are in place, run preprocessing/preprocess_encode.py --mode index for scientific articles, or preprocessing/preprocess_encode.py --mode index-news for news, to encode and save the indexed data in the database.","title":"Dataset preparation"},{"location":"quin-general/","text":"Quin-general This is modified from Quin+ . The Quin-general backend model currently runs on EXALIT workstation. Quick start The Quin-general process is currently managed as a systemd service, with the script located at /etc/systemd/system/quin-general.service . To check if Quin-general is running, run the following command: sudo systemctl status quin-general.service If it is not running, you can start the service by running: sudo systemctl restart quin-general.service To run the script directly for testing, change the directory to then run python quin_web.py --mode=serve Set up from scratch 1) Download the encoder model from here and put it under models/weights/encoder 2) Download the NLI model from here and put it under models/weights/nli 3) Download the passage ranking model from here and put it under models/weights/passage_ranker 4) Run pip install -r backend/requirements.txt Setting up the GPU In backend/quin_web.py you can find the following line: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"...\" Depending on which GPUs are available on the machine, you should set this variable to at least one avaiable GPU id. One GPU is sufficient to run the system. For example, if GPU number 3 is available then you can set it as follows: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" A nice way to check the availability of the GPUs is using the gpustat command. You can install this tool using pip: pip install gpustat","title":"Quin-general"},{"location":"quin-general/#quin-general","text":"This is modified from Quin+ . The Quin-general backend model currently runs on EXALIT workstation.","title":"Quin-general"},{"location":"quin-general/#quick-start","text":"The Quin-general process is currently managed as a systemd service, with the script located at /etc/systemd/system/quin-general.service . To check if Quin-general is running, run the following command: sudo systemctl status quin-general.service If it is not running, you can start the service by running: sudo systemctl restart quin-general.service To run the script directly for testing, change the directory to then run python quin_web.py --mode=serve","title":"Quick start"},{"location":"quin-general/#set-up-from-scratch","text":"1) Download the encoder model from here and put it under models/weights/encoder 2) Download the NLI model from here and put it under models/weights/nli 3) Download the passage ranking model from here and put it under models/weights/passage_ranker 4) Run pip install -r backend/requirements.txt","title":"Set up from scratch"},{"location":"quin-general/#setting-up-the-gpu","text":"In backend/quin_web.py you can find the following line: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"...\" Depending on which GPUs are available on the machine, you should set this variable to at least one avaiable GPU id. One GPU is sufficient to run the system. For example, if GPU number 3 is available then you can set it as follows: os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" A nice way to check the availability of the GPUs is using the gpustat command. You can install this tool using pip: pip install gpustat","title":"Setting up the GPU"},{"location":"twatch-crawler/","text":"Twatch tweet crawler This folder contains scripts that download the \"dehydrated\" version of COVID-19 related tweets from The Panacea Lab dataset , hydrate them, and store the relevant information in an SQLite database. Current status The dataset has stopped updating as of 15 April 2023. The crawler script is located on A100. The service is managed by /etc/init.d/tweet-crawler . To check crawling status, go to $HOME/logs . The service should always be running; the script itself will sleep for 24 hours after downloading the latest data. When the script has finished downloading a new batch of data, an empty file named new_tweets_available will be generated. EXALIT searches for this file every day at 2am using a cron job. If new updates are found, EXALIT will copy the Twatch DB from A100 and restart Twatch backend. In the event that the tweet-crawler service is down, the script dgx-a100-status-check.sh on EXALIT should detect it within 20 minutes and restart it. Refer to misc/dgx-a100-status-check.sh . What each file does crawl.sh : sets up & clean up the log folder, calls the Python crawler script, and sleeps for 24 hours before checking for new update again. This script takes care of the entire crawling process and is intended to be used in a systemd service unit file, or other similar service managers. init.sql : the database schema of the output SQLITE database. It should be used in the event of creating a new DB from scratch. last_processed_date : keeps track of what was the latest set of daily tweets downloaded from Panacea Lab repository. Don't delete. processed_chunks : keeps track of which chunk of a single dataset file has been processed. This was meant for big tsv files containing all historical tweets, which were >1GB in size, but is currently still in use, so do not delete. requirements.txt : install required Python packages: pip install -r requirements.txt tsv_to_db.py : the main script that does the downloading, processing and storing of processed tweets. tsv_to_postgres_[errors_]log : stores output from tsv_to_db.py . The output database used to be Postgres hence the filename. twitter_auth_info.py : put Twitter API keys here whitelist : tweets from accounts whose handles are found here will not be left out even though they do not get enough likes/retweets. Currently empty but still used in script, so do not delete.","title":"Crawler"},{"location":"twatch-crawler/#twatch-tweet-crawler","text":"This folder contains scripts that download the \"dehydrated\" version of COVID-19 related tweets from The Panacea Lab dataset , hydrate them, and store the relevant information in an SQLite database.","title":"Twatch tweet crawler"},{"location":"twatch-crawler/#current-status","text":"The dataset has stopped updating as of 15 April 2023. The crawler script is located on A100. The service is managed by /etc/init.d/tweet-crawler . To check crawling status, go to $HOME/logs . The service should always be running; the script itself will sleep for 24 hours after downloading the latest data. When the script has finished downloading a new batch of data, an empty file named new_tweets_available will be generated. EXALIT searches for this file every day at 2am using a cron job. If new updates are found, EXALIT will copy the Twatch DB from A100 and restart Twatch backend. In the event that the tweet-crawler service is down, the script dgx-a100-status-check.sh on EXALIT should detect it within 20 minutes and restart it. Refer to misc/dgx-a100-status-check.sh .","title":"Current status"},{"location":"twatch-crawler/#what-each-file-does","text":"crawl.sh : sets up & clean up the log folder, calls the Python crawler script, and sleeps for 24 hours before checking for new update again. This script takes care of the entire crawling process and is intended to be used in a systemd service unit file, or other similar service managers. init.sql : the database schema of the output SQLITE database. It should be used in the event of creating a new DB from scratch. last_processed_date : keeps track of what was the latest set of daily tweets downloaded from Panacea Lab repository. Don't delete. processed_chunks : keeps track of which chunk of a single dataset file has been processed. This was meant for big tsv files containing all historical tweets, which were >1GB in size, but is currently still in use, so do not delete. requirements.txt : install required Python packages: pip install -r requirements.txt tsv_to_db.py : the main script that does the downloading, processing and storing of processed tweets. tsv_to_postgres_[errors_]log : stores output from tsv_to_db.py . The output database used to be Postgres hence the filename. twitter_auth_info.py : put Twitter API keys here whitelist : tweets from accounts whose handles are found here will not be left out even though they do not get enough likes/retweets. Currently empty but still used in script, so do not delete.","title":"What each file does"},{"location":"twatch/","text":"Overview Twatch is an interactive web system designed for topic propagation analysis on Twitter. A demo can be accessed here (under the Twitter tab) . Deployment Prerequisites for Production Server Docker A Linux server is preferred, as scripts are written for the Linux shell only. Steps Twatch is deployed using Docker. Follow these steps: Ensure that the script prepare_deploy_image.sh is executable. If unsure, run: chmod +x prepare_deploy_image.sh Use the following command to generate Docker images: ./prepare_deploy_image.sh -[f|b|d] The options f , d , and b refer to the three components: frontend, backend, and database. If you only want to build the Docker images for the frontend and backend, you can run: ./prepare_deploy_image.sh -fb The database has been migrated to an SQLite file. This script, along with all other scripts mentioned below, retains the lines dealing with the Postgres container for reference only. After the script exits successfully with the message Finished creating deployment file , the Docker images will be available in the archive file twatch-<DATE>.tar.gz . Transfer this file to the production server (currently the NUS VM server), along with the helper scripts deploy.sh and docker-run.sh . Make the scripts executable: chmod +x *.sh Load the docker images on the production server using the script deploy.sh Start the docker containers by running the script docker-run.sh The script assumes that there is a docker network named letscheck-network , since this is the one set up earlier on the NUS VM. For deployment on other platforms, this should be changed. The containers should be connected to the same docker network because they need to talk to each other. Similarly, the port values are based on NUS VM configurations. For other platforms, modify them accordingly, especially the outbound port of twatch-frontend . Maintenance The tweet crawler runs on the DGX A100 workstation. It checks for new daily tweet IDs from the Panacea Lab dataset since last update, and uses Twitter API to get fully hydrated tweet information. The crawler stores data in an SQLite database file. The crawler has stopped running as Panacea Lab dataset has stopped updating since 15 April 2023. Development Prerequisite Node.js and Yarn for package management postgres Database The database creation script is available at scripts/init_with_fts.sql . Setup Run yarn install in this folder to install the required node packages. Because frontend and backend are developed independently, I didn't write a script to run both in parallel. To run backend, change directory to backend/ and execute node index.js . The default port is 8080. To run frontend, change directory to frontend/ and execute yarn run start . The default port is 3000. Twatch should be available at http://localhost:3000/","title":"Application"},{"location":"twatch/#overview","text":"Twatch is an interactive web system designed for topic propagation analysis on Twitter. A demo can be accessed here (under the Twitter tab) .","title":"Overview"},{"location":"twatch/#deployment","text":"","title":"Deployment"},{"location":"twatch/#prerequisites-for-production-server","text":"Docker A Linux server is preferred, as scripts are written for the Linux shell only.","title":"Prerequisites for Production Server"},{"location":"twatch/#steps","text":"Twatch is deployed using Docker. Follow these steps: Ensure that the script prepare_deploy_image.sh is executable. If unsure, run: chmod +x prepare_deploy_image.sh Use the following command to generate Docker images: ./prepare_deploy_image.sh -[f|b|d] The options f , d , and b refer to the three components: frontend, backend, and database. If you only want to build the Docker images for the frontend and backend, you can run: ./prepare_deploy_image.sh -fb The database has been migrated to an SQLite file. This script, along with all other scripts mentioned below, retains the lines dealing with the Postgres container for reference only. After the script exits successfully with the message Finished creating deployment file , the Docker images will be available in the archive file twatch-<DATE>.tar.gz . Transfer this file to the production server (currently the NUS VM server), along with the helper scripts deploy.sh and docker-run.sh . Make the scripts executable: chmod +x *.sh Load the docker images on the production server using the script deploy.sh Start the docker containers by running the script docker-run.sh The script assumes that there is a docker network named letscheck-network , since this is the one set up earlier on the NUS VM. For deployment on other platforms, this should be changed. The containers should be connected to the same docker network because they need to talk to each other. Similarly, the port values are based on NUS VM configurations. For other platforms, modify them accordingly, especially the outbound port of twatch-frontend .","title":"Steps"},{"location":"twatch/#maintenance","text":"The tweet crawler runs on the DGX A100 workstation. It checks for new daily tweet IDs from the Panacea Lab dataset since last update, and uses Twitter API to get fully hydrated tweet information. The crawler stores data in an SQLite database file. The crawler has stopped running as Panacea Lab dataset has stopped updating since 15 April 2023.","title":"Maintenance"},{"location":"twatch/#development","text":"","title":"Development"},{"location":"twatch/#prerequisite","text":"Node.js and Yarn for package management postgres","title":"Prerequisite"},{"location":"twatch/#database","text":"The database creation script is available at scripts/init_with_fts.sql .","title":"Database"},{"location":"twatch/#setup","text":"Run yarn install in this folder to install the required node packages. Because frontend and backend are developed independently, I didn't write a script to run both in parallel. To run backend, change directory to backend/ and execute node index.js . The default port is 8080. To run frontend, change directory to frontend/ and execute yarn run start . The default port is 3000. Twatch should be available at http://localhost:3000/","title":"Setup"}]}